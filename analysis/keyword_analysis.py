#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…³é”®è¯åˆ†ææ¨¡å—
è¯»å–å°çº¢ä¹¦ç¬”è®°æ•°æ®ï¼Œåˆ†ææ ‡é¢˜ä¸­çš„å…³é”®è¯é¢‘ç‡ï¼Œç”Ÿæˆè¯äº‘å›¾
"""

import os
import sys
import argparse
import pandas as pd
import jieba
import jieba.analyse
from collections import Counter
from datetime import datetime
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np


def setup_jieba():
    """è®¾ç½® jieba åˆ†è¯"""
    # æ·»åŠ è‡ªå®šä¹‰è¯å…¸
    custom_words = [
        'æ™®æ‹‰æ', 'ç‘œä¼½', 'å¥èº«', 'å‡è‚¥', 'å¡‘å½¢', 'ä½“æ€', 'æ ¸å¿ƒ',
        'å°çº¢ä¹¦', 'ç§è‰', 'æµ‹è¯„', 'æ¨è', 'åˆ†äº«', 'ä½“éªŒ',
        'æ•™ç¨‹', 'å…¥é—¨', 'è¿›é˜¶', 'ä¸“ä¸š', 'å™¨æ¢°', 'å«ä¸Š',
        'ç§æ•™', 'è¯¾ç¨‹', 'è®­ç»ƒ', 'è¿åŠ¨', 'åº·å¤'
    ]
    
    for word in custom_words:
        jieba.add_word(word)
    
    # è®¾ç½®åœç”¨è¯
    stop_words = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
        'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
        'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ',
        'å°±æ˜¯', 'è¿˜æ˜¯', 'æ¯”è¾ƒ', 'éå¸¸', 'ç‰¹åˆ«', 'çœŸçš„', 'å¯ä»¥', 'ä½†æ˜¯',
        'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶å', 'è¿˜æœ‰', 'æˆ–è€…', 'ä»¥åŠ',
        'ä»¥ä¸º', 'è§‰å¾—', 'æ„Ÿè§‰', 'åº”è¯¥', 'å¯èƒ½', 'ä¸€å®š', 'è‚¯å®š', 'ç»å¯¹',
        'å®Œå…¨', 'åŸºæœ¬', 'ä¸»è¦', 'é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ', 'åŸºç¡€', 'ç®€å•',
        'å¤æ‚', 'å›°éš¾', 'å®¹æ˜“', 'æ–¹ä¾¿', 'éº»çƒ¦', 'é—®é¢˜', 'æ–¹æ³•', 'æ–¹å¼',
        'æ—¶å€™', 'å¼€å§‹', 'ç»“æŸ', 'è¿‡ç¨‹', 'ç»“æœ', 'æ•ˆæœ', 'ä½œç”¨', 'åŠŸèƒ½',
        'ç‰¹ç‚¹', 'ä¼˜ç‚¹', 'ç¼ºç‚¹', 'å¥½å¤„', 'åå¤„', 'å½±å“', 'å˜åŒ–', 'æé«˜',
        'å¢åŠ ', 'å‡å°‘', 'ä¿æŒ', 'ç»´æŒ', 'ç»§ç»­', 'åœæ­¢', 'å¼€å§‹', 'ç»“æŸ'
    }
    
    return stop_words


def extract_keywords_from_titles(titles, stop_words, top_n=30):
    """ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯"""
    print(f"ğŸ“Š å¼€å§‹åˆ†æ {len(titles)} ä¸ªæ ‡é¢˜...")
    
    # åˆå¹¶æ‰€æœ‰æ ‡é¢˜
    all_text = ' '.join(titles)
    
    # ä½¿ç”¨ jieba åˆ†è¯
    words = jieba.cut(all_text)
    
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—ç¬¦
    filtered_words = [
        word.strip() for word in words 
        if len(word.strip()) > 1 and word.strip() not in stop_words
    ]
    
    # ç»Ÿè®¡è¯é¢‘
    word_counter = Counter(filtered_words)
    
    # è·å–å‰ N ä¸ªå…³é”®è¯
    top_keywords = word_counter.most_common(top_n)
    
    print(f"âœ… æå–åˆ° {len(top_keywords)} ä¸ªå…³é”®è¯")
    
    return top_keywords, word_counter


def save_keywords_csv(keywords, output_path):
    """ä¿å­˜å…³é”®è¯åˆ†æç»“æœåˆ° CSV"""
    df = pd.DataFrame(keywords, columns=['å…³é”®è¯', 'å‡ºç°æ¬¡æ•°'])
    df['æ’å'] = range(1, len(df) + 1)
    df = df[['æ’å', 'å…³é”®è¯', 'å‡ºç°æ¬¡æ•°']]
    
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ å…³é”®è¯åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return df


def generate_wordcloud(word_counter, output_path, max_words=100):
    """ç”Ÿæˆè¯äº‘å›¾"""
    print("ğŸ¨ ç”Ÿæˆè¯äº‘å›¾...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    font_path = None
    possible_fonts = [
        'C:/Windows/Fonts/simhei.ttf',  # Windows é»‘ä½“
        'C:/Windows/Fonts/msyh.ttf',    # Windows å¾®è½¯é›…é»‘
        '/System/Library/Fonts/PingFang.ttc',  # macOS
        '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'  # Linux
    ]
    
    for font in possible_fonts:
        if os.path.exists(font):
            font_path = font
            break
    
    # åˆ›å»ºè¯äº‘
    wordcloud = WordCloud(
        width=1200,
        height=800,
        background_color='white',
        font_path=font_path,
        max_words=max_words,
        colormap='viridis',
        relative_scaling=0.5,
        random_state=42
    ).generate_from_frequencies(word_counter)
    
    # ä¿å­˜è¯äº‘å›¾
    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('å°çº¢ä¹¦ç¬”è®°æ ‡é¢˜å…³é”®è¯è¯äº‘å›¾', fontsize=16, pad=20)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ¨ è¯äº‘å›¾å·²ä¿å­˜åˆ°: {output_path}")


def analyze_keyword_trends(df, keywords_df, output_dir):
    """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
    print("ğŸ“ˆ åˆ†æå…³é”®è¯è¶‹åŠ¿...")
    
    # è½¬æ¢æ—¶é—´æˆ³
    df['publish_date'] = pd.to_datetime(df['time'], unit='ms')
    df['month'] = df['publish_date'].dt.to_period('M')
    
    # è·å–å‰10ä¸ªå…³é”®è¯
    top_10_keywords = keywords_df.head(10)['å…³é”®è¯'].tolist()
    
    # åˆ†ææ¯ä¸ªæœˆçš„å…³é”®è¯å‡ºç°æƒ…å†µ
    monthly_trends = []
    
    for month in df['month'].unique():
        month_data = df[df['month'] == month]
        month_titles = ' '.join(month_data['title'].fillna(''))
        
        month_row = {'æœˆä»½': str(month)}
        for keyword in top_10_keywords:
            count = month_titles.count(keyword)
            month_row[keyword] = count
        
        monthly_trends.append(month_row)
    
    trends_df = pd.DataFrame(monthly_trends)
    trends_output = os.path.join(output_dir, 'keyword_trends.csv')
    trends_df.to_csv(trends_output, index=False, encoding='utf-8-sig')
    
    print(f"ğŸ“ˆ å…³é”®è¯è¶‹åŠ¿åˆ†æå·²ä¿å­˜åˆ°: {trends_output}")
    
    return trends_df


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å°çº¢ä¹¦ç¬”è®°å…³é”®è¯åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python analysis/keyword_analysis.py --input core/media_crawler/data/xhs/1_search_contents_2025-07-02.csv
  python analysis/keyword_analysis.py --input data.csv --top-n 50 --max-words 150
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        required=True,
        help='è¾“å…¥çš„ CSV æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='output',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: output)'
    )
    parser.add_argument(
        '--top-n', '-n',
        type=int,
        default=30,
        help='æå–å‰ N ä¸ªå…³é”®è¯ (é»˜è®¤: 30)'
    )
    parser.add_argument(
        '--max-words',
        type=int,
        default=100,
        help='è¯äº‘å›¾æœ€å¤§è¯æ•° (é»˜è®¤: 100)'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("=" * 60)
    print("ğŸ” å°çº¢ä¹¦ç¬”è®°å…³é”®è¯åˆ†æ")
    print("=" * 60)
    
    try:
        # è¯»å–æ•°æ®
        print(f"ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶: {args.input}")
        df = pd.read_csv(args.input)
        
        if 'title' not in df.columns:
            print("âŒ CSV æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'title' åˆ—")
            sys.exit(1)
        
        # æ¸…ç†æ ‡é¢˜æ•°æ®
        titles = df['title'].fillna('').astype(str).tolist()
        titles = [title.strip() for title in titles if title.strip()]
        
        if not titles:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ ‡é¢˜æ•°æ®")
            sys.exit(1)
        
        # è®¾ç½® jieba
        stop_words = setup_jieba()
        
        # æå–å…³é”®è¯
        top_keywords, word_counter = extract_keywords_from_titles(
            titles, stop_words, args.top_n
        )
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å…³é”®è¯ CSV
        keywords_output = os.path.join(
            args.output_dir, f'keywords_analysis_{timestamp}.csv'
        )
        keywords_df = save_keywords_csv(top_keywords, keywords_output)
        
        # ç”Ÿæˆè¯äº‘å›¾
        wordcloud_output = os.path.join(
            args.output_dir, f'wordcloud_{timestamp}.png'
        )
        generate_wordcloud(word_counter, wordcloud_output, args.max_words)
        
        # åˆ†æå…³é”®è¯è¶‹åŠ¿
        if 'time' in df.columns:
            analyze_keyword_trends(df, keywords_df, args.output_dir)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("ğŸ“Š åˆ†æç»“æœç»Ÿè®¡")
        print("=" * 60)
        print(f"ğŸ“ åˆ†æç¬”è®°æ•°é‡: {len(titles)}")
        print(f"ğŸ”¤ æå–å…³é”®è¯æ•°é‡: {len(top_keywords)}")
        print(f"ğŸ“„ å…³é”®è¯ CSV: {keywords_output}")
        print(f"ğŸ¨ è¯äº‘å›¾: {wordcloud_output}")
        
        print("\nğŸ† å‰10ä¸ªçƒ­é—¨å…³é”®è¯:")
        for i, (word, count) in enumerate(top_keywords[:10], 1):
            print(f"  {i:2d}. {word} ({count}æ¬¡)")
        
        print("\nâœ… å…³é”®è¯åˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
