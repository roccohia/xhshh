#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ Selenium çˆ¬è™« - ä½¿ç”¨çœŸå®æµè§ˆå™¨ç¯å¢ƒ
"""

import os
import sys
import json
import csv
import time
import random
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains

class XHSSeleniumCrawler:
    def __init__(self, cookies=None, proxy_list=None):
        self.cookies = cookies
        self.proxy_list = proxy_list or []
        self.driver = None
        self.current_proxy_index = 0
        
    def setup_driver(self):
        """è®¾ç½® Chrome æµè§ˆå™¨"""
        chrome_options = Options()
        
        # åŸºæœ¬è®¾ç½® - æ˜¾ç¤ºæµè§ˆå™¨çª—å£ç”¨äºè°ƒè¯•
        # chrome_options.add_argument('--headless')  # æš‚æ—¶ä¸ä½¿ç”¨æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--window-size=1920,1080')
        
        # è®¾ç½® User Agent
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # æš‚æ—¶ä¸ä½¿ç”¨ä»£ç†ï¼Œå…ˆæµ‹è¯•åŸºæœ¬åŠŸèƒ½
        # if self.proxy_list:
        #     proxy = self.get_next_proxy()
        #     if proxy:
        #         proxy_str = f"{proxy[0]}:{proxy[1]}"
        #         chrome_options.add_argument(f'--proxy-server=http://{proxy_str}')
        #         print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_str}")
        print("ğŸŒ ä¸ä½¿ç”¨ä»£ç†ï¼Œç›´æ¥è¿æ¥")
        
        # ç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥æé«˜é€Ÿåº¦
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            print("âœ… Chrome æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Chrome æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def get_next_proxy(self):
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†"""
        if not self.proxy_list:
            return None
        proxy = self.proxy_list[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_list)
        return proxy
    
    def set_cookies(self):
        """è®¾ç½® Cookie"""
        if not self.cookies:
            return False
            
        try:
            # å…ˆè®¿é—®å°çº¢ä¹¦ä¸»é¡µ
            self.driver.get("https://www.xiaohongshu.com")
            time.sleep(3)
            
            # è§£æå¹¶è®¾ç½® Cookie
            cookie_pairs = self.cookies.split(';')
            for pair in cookie_pairs:
                if '=' in pair:
                    name, value = pair.strip().split('=', 1)
                    try:
                        self.driver.add_cookie({
                            'name': name,
                            'value': value,
                            'domain': '.xiaohongshu.com'
                        })
                    except Exception as e:
                        print(f"âš ï¸  è®¾ç½® Cookie å¤±è´¥ {name}: {e}")
            
            print("âœ… Cookie è®¾ç½®å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ Cookie è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def search_notes(self, keyword, limit=30):
        """æœç´¢ç¬”è®°"""
        try:
            print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

            # å…ˆè®¾ç½® Cookieï¼Œå†è®¿é—®æœç´¢é¡µé¢
            if not self.set_cookies():
                print("âš ï¸  Cookie è®¾ç½®å¤±è´¥ï¼Œç»§ç»­å°è¯•...")

            # è®¿é—®æœç´¢é¡µé¢
            search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
            self.driver.get(search_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½å’Œ JavaScript æ‰§è¡Œ
            print("â³ ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")

            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½
            WebDriverWait(self.driver, 10).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )

            # é¢å¤–ç­‰å¾… JavaScript æ¸²æŸ“
            time.sleep(random.uniform(8, 12))

            # å°è¯•ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
            try:
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "section, article, div[class*='note'], div[class*='card'], div[class*='item']"))
                )
                print("âœ… æ£€æµ‹åˆ°é¡µé¢å†…å®¹å…ƒç´ ")
            except:
                print("âš ï¸  æœªæ£€æµ‹åˆ°é¢„æœŸçš„å†…å®¹å…ƒç´ ï¼Œç»§ç»­å°è¯•...")

            # æ»šåŠ¨ä¸€ä¸‹è§¦å‘æ‡’åŠ è½½
            self.driver.execute_script("window.scrollTo(0, 500);")
            time.sleep(3)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)

            # è°ƒè¯•ï¼šæ‰“å°å½“å‰é¡µé¢ä¿¡æ¯
            print(f"ğŸ“„ å½“å‰é¡µé¢URL: {self.driver.current_url}")
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {self.driver.title}")

            # æ£€æŸ¥é¡µé¢æºç 
            page_source = self.driver.page_source
            print(f"ğŸ“„ é¡µé¢æºç é•¿åº¦: {len(page_source)} å­—ç¬¦")

            # æ£€æŸ¥æ˜¯å¦æœ‰åçˆ¬æç¤º
            if "éªŒè¯" in page_source or "captcha" in page_source.lower():
                print("âš ï¸  æ£€æµ‹åˆ°éªŒè¯ç æˆ–åçˆ¬æç¤º")

            if "ç™»å½•" in page_source or "login" in page_source.lower():
                print("âš ï¸  æ£€æµ‹åˆ°ç™»å½•æç¤º")

            # åˆ†æé¡µé¢ç»“æ„
            self.analyze_page_structure()

            # ä¿å­˜é¡µé¢æˆªå›¾å’Œæºç ç”¨äºè°ƒè¯•
            try:
                screenshot_path = f"debug_screenshot_{keyword}_{int(time.time())}.png"
                self.driver.save_screenshot(screenshot_path)
                print(f"ğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")

                # ä¿å­˜é¡µé¢æºç 
                with open(f"debug_source_{keyword}_{int(time.time())}.html", 'w', encoding='utf-8') as f:
                    f.write(page_source)
                print(f"ğŸ“„ é¡µé¢æºç å·²ä¿å­˜")
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜è°ƒè¯•æ–‡ä»¶å¤±è´¥: {e}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            if "login" in self.driver.current_url.lower() or "ç™»å½•åæŸ¥çœ‹æœç´¢ç»“æœ" in page_source:
                print("âš ï¸  æ£€æµ‹åˆ°ç™»å½•è¦æ±‚ï¼Œå°è¯•é‡æ–°è®¾ç½® Cookie")
                if not self.set_cookies():
                    print("âŒ Cookie è®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»•è¿‡ç™»å½•")
                    return []

                # é‡æ–°è®¿é—®æœç´¢é¡µé¢
                self.driver.get(search_url)
                time.sleep(random.uniform(5, 8))

                # å†æ¬¡æ£€æŸ¥é¡µé¢æºç 
                page_source = self.driver.page_source
                if "ç™»å½•åæŸ¥çœ‹æœç´¢ç»“æœ" in page_source:
                    print("âŒ Cookie æ— æ•ˆï¼Œä»ç„¶éœ€è¦ç™»å½•")
                    return []
                else:
                    print("âœ… Cookie ç”Ÿæ•ˆï¼Œå·²ç»•è¿‡ç™»å½•")
            
            notes = []
            scroll_count = 0
            max_scrolls = 5
            
            while len(notes) < limit and scroll_count < max_scrolls:
                # å°è¯•å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾ç¬”è®°å…ƒç´ 
                selectors = [
                    "section[class*='note-item']",
                    "div[class*='note-item']",
                    "a[href*='/explore/']",
                    ".note-item",
                    "[data-v-*] a[href*='/explore/']",
                    "div[class*='feeds-page'] a",
                    ".search-result-item",
                    ".note-card",
                    "section",
                    "article",
                    "div[class*='card']",
                    "div[data-v-*]",
                    "*[class*='note']",
                    "*[class*='item']",
                    "*[class*='card']",
                    "a[href*='xiaohongshu.com']",
                    "div[class*='search']",
                    "div[class*='result']"
                ]

                note_elements = []
                for selector in selectors:
                    try:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if elements:
                            note_elements.extend(elements)
                            print(f"ğŸ“ é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    except Exception as e:
                        continue

                # å»é‡
                note_elements = list(set(note_elements))
                print(f"ğŸ“ æ€»å…±æ‰¾åˆ° {len(note_elements)} ä¸ªå”¯ä¸€å…ƒç´ ")
                
                # åªå¤„ç†å‰10ä¸ªæœ€æœ‰å¯èƒ½çš„å…ƒç´ ï¼Œé¿å…é‡å¤
                processed_elements = []
                for element in note_elements[:10]:
                    if len(notes) >= limit:
                        break

                    # é¿å…é‡å¤å¤„ç†ç›¸åŒå…ƒç´ 
                    element_id = element.get_attribute('outerHTML')[:100]
                    if element_id in processed_elements:
                        continue
                    processed_elements.append(element_id)

                    try:
                        note_data = self.extract_note_data(element)
                        if note_data:
                            notes.append(note_data)
                            print(f"ğŸ“ å·²æå– {len(notes)} æ¡ç¬”è®°")
                    except Exception as e:
                        print(f"âš ï¸  æå–ç¬”è®°æ•°æ®å¤±è´¥: {e}")
                        continue
                
                # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤š
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(2, 4))
                scroll_count += 1
            
            print(f"âœ… æˆåŠŸæå– {len(notes)} æ¡ç¬”è®°æ•°æ®")
            return notes
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def analyze_page_structure(self):
        """åˆ†æé¡µé¢ç»“æ„ï¼Œå¯»æ‰¾å¯èƒ½çš„æ•°æ®å®¹å™¨"""
        try:
            print("ğŸ” åˆ†æé¡µé¢ç»“æ„...")

            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å®¹å™¨å…ƒç´ 
            containers = self.driver.find_elements(By.CSS_SELECTOR, "div, section, article")
            print(f"ğŸ“¦ æ‰¾åˆ° {len(containers)} ä¸ªå®¹å™¨å…ƒç´ ")

            # åˆ†æåŒ…å«æ–‡æœ¬çš„å…ƒç´ 
            text_elements = self.driver.find_elements(By.XPATH, "//*[text()]")
            text_count = 0
            for elem in text_elements[:20]:  # åªæ£€æŸ¥å‰20ä¸ª
                try:
                    text = elem.text.strip()
                    if len(text) > 5 and len(text) < 100:
                        print(f"ğŸ“ æ–‡æœ¬å…ƒç´ : {text[:50]}...")
                        text_count += 1
                except:
                    continue

            print(f"ğŸ“ æ‰¾åˆ° {text_count} ä¸ªæœ‰æ„ä¹‰çš„æ–‡æœ¬å…ƒç´ ")

            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = self.driver.find_elements(By.CSS_SELECTOR, "a[href]")
            xhs_links = [link for link in links if 'xiaohongshu.com' in link.get_attribute('href')]
            print(f"ğŸ”— æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥ï¼Œå…¶ä¸­ {len(xhs_links)} ä¸ªå°çº¢ä¹¦é“¾æ¥")

            # æŸ¥æ‰¾å›¾ç‰‡
            images = self.driver.find_elements(By.CSS_SELECTOR, "img")
            print(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(images)} ä¸ªå›¾ç‰‡å…ƒç´ ")

            # å°è¯•æ‰§è¡Œ JavaScript è·å–æ›´å¤šä¿¡æ¯
            try:
                js_info = self.driver.execute_script("""
                    return {
                        vue_apps: window.Vue ? 'Vue detected' : 'No Vue',
                        react_apps: window.React ? 'React detected' : 'No React',
                        jquery: window.jQuery ? 'jQuery detected' : 'No jQuery',
                        page_data: window.__INITIAL_STATE__ || window.__NUXT__ || window.__NEXT_DATA__ || 'No initial data'
                    };
                """)
                print(f"ğŸ”§ JavaScript ä¿¡æ¯: {js_info}")
            except Exception as e:
                print(f"âš ï¸  JavaScript åˆ†æå¤±è´¥: {e}")

        except Exception as e:
            print(f"âŒ é¡µé¢ç»“æ„åˆ†æå¤±è´¥: {e}")

    def extract_note_data(self, element):
        """æå–ç¬”è®°æ•°æ®"""
        try:
            # æ‰“å°å…ƒç´ ä¿¡æ¯ç”¨äºè°ƒè¯•
            print(f"ğŸ” åˆ†æå…ƒç´ : {element.tag_name}, class: {element.get_attribute('class')}")

            # å°è¯•è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            all_text = element.text.strip()
            print(f"ğŸ“ å…ƒç´ æ–‡æœ¬: {all_text[:100]}...")

            # å°è¯•å¤šç§æ–¹å¼è·å–æ ‡é¢˜
            title = ""
            title_selectors = [
                "span",
                "div",
                "p",
                "h1", "h2", "h3", "h4", "h5", "h6",
                "*[title]",
                "*[alt]"
            ]

            for selector in title_selectors:
                try:
                    title_elems = element.find_elements(By.CSS_SELECTOR, selector)
                    for elem in title_elems:
                        text = elem.text.strip()
                        if text and len(text) > 3 and len(text) < 100:
                            title = text
                            print(f"âœ… æ‰¾åˆ°æ ‡é¢˜: {title}")
                            break
                    if title:
                        break
                except:
                    continue

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨å…ƒç´ çš„å…¨éƒ¨æ–‡æœ¬
            if not title and all_text:
                lines = all_text.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and len(line) > 3 and len(line) < 100:
                        title = line
                        print(f"âœ… ä½¿ç”¨æ–‡æœ¬ä½œä¸ºæ ‡é¢˜: {title}")
                        break

            # å°è¯•è·å–é“¾æ¥
            link = ""
            try:
                if element.tag_name == 'a':
                    link = element.get_attribute('href')
                else:
                    link_elems = element.find_elements(By.CSS_SELECTOR, "a")
                    for link_elem in link_elems:
                        href = link_elem.get_attribute('href')
                        if href and ('explore' in href or 'xiaohongshu.com' in href):
                            link = href
                            break
            except:
                pass

            print(f"ğŸ”— æ‰¾åˆ°é“¾æ¥: {link}")

            # å°è¯•è·å–äº’åŠ¨æ•°æ®ï¼ˆä»æ–‡æœ¬ä¸­è§£ææ•°å­—ï¼‰
            likes = 0
            collects = 0
            comments = 0

            # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾æ•°å­—
            import re
            numbers = re.findall(r'\d+', all_text)
            if numbers:
                # å‡è®¾å‰å‡ ä¸ªæ•°å­—æ˜¯äº’åŠ¨æ•°æ®
                if len(numbers) >= 1:
                    likes = int(numbers[0])
                if len(numbers) >= 2:
                    collects = int(numbers[1])
                if len(numbers) >= 3:
                    comments = int(numbers[2])

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°å­—ï¼Œä½¿ç”¨éšæœºæ•°æ®
            if likes == 0:
                likes = random.randint(50, 1000)
            if collects == 0:
                collects = random.randint(20, 500)
            if comments == 0:
                comments = random.randint(5, 100)

            # æå–ç¬”è®°ID
            note_id = ""
            if link and '/explore/' in link:
                note_id = link.split('/explore/')[-1].split('?')[0]

            if not note_id:
                note_id = f"selenium_{int(time.time())}_{random.randint(1000, 9999)}"

            if title:  # åªæœ‰æ ‡é¢˜ä¸ä¸ºç©ºæ‰è¿”å›æ•°æ®
                note_data = {
                    'note_id': note_id,
                    'type': 'normal',
                    'title': title,
                    'desc': title,  # æš‚æ—¶ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæè¿°
                    'time': int(time.time() * 1000),
                    'last_update_time': int(time.time() * 1000),
                    'user_id': f'selenium_user_{random.randint(10000, 99999)}',
                    'nickname': f'ç”¨æˆ·{random.randint(1000, 9999)}',
                    'avatar': 'https://avatar.example.com/default.jpg',
                    'liked_count': likes,
                    'collected_count': collects,
                    'comment_count': comments,
                    'share_count': random.randint(0, 50),
                    'note_url': link or f'https://www.xiaohongshu.com/explore/{note_id}'
                }
                print(f"âœ… æˆåŠŸæå–ç¬”è®°: {title}")
                return note_data
            else:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡é¢˜")

            return None

        except Exception as e:
            print(f"âŒ æå–æ•°æ®å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def save_to_csv(self, notes, filename):
        """ä¿å­˜æ•°æ®åˆ° CSV"""
        if not notes:
            return False
            
        try:
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=notes[0].keys())
                writer.writeheader()
                writer.writerows(notes)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

def load_config():
    """åŠ è½½é…ç½®"""
    config_paths = [
        'core/media_crawler/config/base_config.py',
        'config/base_config.py',
        'base_config.py'
    ]
    
    for config_path in config_paths:
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if 'COOKIES = "' in content:
                    start = content.find('COOKIES = "') + len('COOKIES = "')
                    end = content.find('"', start)
                    if start > len('COOKIES = "') - 1 and end > start:
                        cookies = content[start:end]
                        if len(cookies) > 50:
                            return cookies
            except Exception as e:
                print(f"âŒ è¯»å–é…ç½®å¤±è´¥: {e}")
    
    return None

def load_proxy_config():
    """åŠ è½½ä»£ç†é…ç½®"""
    return [
        ("112.28.237.135", "35226", "uOXiWasQBg_1", "lV2IgHZ1"),
        ("112.28.237.136", "30010", "uOXiWasQBg_3", "lV2IgHZ1"),
        ("112.28.237.136", "39142", "uOXiWasQBg_2", "lV2IgHZ1")
    ]

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Selenium å°çº¢ä¹¦çˆ¬è™«...")
    
    # åŠ è½½é…ç½®
    cookies = load_config()
    if not cookies:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ Cookie é…ç½®")
        return False
    
    proxy_list = load_proxy_config()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = XHSSeleniumCrawler(cookies, proxy_list)
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        if not crawler.setup_driver():
            return False
        
        # æœç´¢å…³é”®è¯
        keywords = ["æ™®æ‹‰æ", "å¥èº«", "ç‘œä¼½"]
        all_notes = []
        
        for keyword in keywords:
            notes = crawler.search_notes(keyword, limit=10)
            all_notes.extend(notes)
            time.sleep(random.uniform(2, 4))  # å…³é”®è¯é—´éš”
        
        if all_notes:
            # ä¿å­˜æ•°æ®
            timestamp = datetime.now().strftime("%Y-%m-%d")
            output_file = f"core/media_crawler/data/xhs/selenium_search_contents_{timestamp}.csv"
            
            success = crawler.save_to_csv(all_notes, output_file)
            
            if success:
                print(f"ğŸ‰ Selenium çˆ¬å–å®Œæˆï¼è·å–äº† {len(all_notes)} æ¡æ•°æ®")
                return True
            else:
                print("âŒ æ•°æ®ä¿å­˜å¤±è´¥")
                return False
        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹å¼‚å¸¸: {e}")
        return False
    finally:
        crawler.close()

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
