#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ç›´æ¥çˆ¬è™« - é€‚ç”¨äº GitHub Actions ç¯å¢ƒ
ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨å°çº¢ä¹¦ API
"""

import os
import sys
import json
import csv
import time
import random
import requests
from datetime import datetime
from urllib.parse import urlencode


class XHSDirectCrawler:
    def __init__(self, cookies):
        self.session = requests.Session()
        self.cookie_string = cookies  # ä¿å­˜åŸå§‹å­—ç¬¦ä¸²
        self.cookies = self.parse_cookies(cookies)
        self.session.cookies.update(self.cookies)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.xiaohongshu.com/',
            'Origin': 'https://www.xiaohongshu.com',
            'X-Requested-With': 'XMLHttpRequest',
        })
    
    def parse_cookies(self, cookie_string):
        """è§£æ Cookie å­—ç¬¦ä¸²"""
        cookies = {}
        if cookie_string:
            for item in cookie_string.split(';'):
                if '=' in item:
                    key, value = item.strip().split('=', 1)
                    cookies[key] = value
        return cookies
    
    def search_notes(self, keyword, limit=50):
        """æœç´¢ç¬”è®°"""
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

        notes = []

        # åªå°è¯•çœŸå®çˆ¬å–ï¼Œä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        print("ğŸš€ å°è¯•çœŸå®æ•°æ®çˆ¬å–...")
        try:
            real_notes = self.try_real_crawl(keyword, limit)
            if real_notes and len(real_notes) > 0:
                print(f"âœ… æˆåŠŸè·å– {len(real_notes)} æ¡çœŸå®æ•°æ®")
                return real_notes
            else:
                print(f"âŒ å…³é”®è¯ '{keyword}' æœªè·å–åˆ°ä»»ä½•çœŸå®æ•°æ®")
                return []
        except Exception as e:
            print(f"âŒ çœŸå®çˆ¬å–å¤±è´¥: {e}")
            return []

        # æœ¬åœ°ç¯å¢ƒå°è¯•çœŸå®è¯·æ±‚
        page = 1

        while len(notes) < limit:
            try:
                # ä½¿ç”¨æ›´ç®€å•çš„æœç´¢æ–¹å¼
                search_url = "https://www.xiaohongshu.com/web_api/sns/v3/page/notes"

                params = {
                    'keyword': keyword,
                    'page': page,
                    'page_size': min(20, limit - len(notes)),
                    'sort': 'time',  # æŒ‰æ—¶é—´æ’åº
                }

                print(f"ğŸ“„ è¯·æ±‚ç¬¬ {page} é¡µ...")

                response = self.session.get(search_url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if data.get('success') and data.get('data'):
                        items = data['data'].get('items', [])
                        
                        if not items:
                            print("ğŸ“„ æ²¡æœ‰æ›´å¤šæ•°æ®")
                            break
                        
                        for item in items:
                            if len(notes) >= limit:
                                break
                            
                            note_data = self.extract_note_data(item)
                            if note_data:
                                notes.append(note_data)
                        
                        print(f"âœ… ç¬¬ {page} é¡µè·å– {len(items)} æ¡æ•°æ®")
                        page += 1
                        
                        # éšæœºå»¶è¿Ÿ
                        time.sleep(random.uniform(1, 3))
                    else:
                        print(f"âŒ API è¿”å›é”™è¯¯: {data}")
                        break
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                    print(f"å“åº”å†…å®¹: {response.text[:200]}")
                    break
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                break
        
        print(f"ğŸ‰ æ€»å…±è·å– {len(notes)} æ¡ç¬”è®°æ•°æ®")
        return notes

    def try_real_crawl(self, keyword, limit):
        """å°è¯•çœŸå®çˆ¬å–æ•°æ®"""
        import requests
        import time
        import random

        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.xiaohongshu.com/',
            'Cookie': self.cookie_string,
            'X-Requested-With': 'XMLHttpRequest',
        }

        # å°è¯•å¤šä¸ªå¯èƒ½çš„ API ç«¯ç‚¹
        search_urls = [
            "https://edith.xiaohongshu.com/api/sns/web/v1/search/notes",
            "https://www.xiaohongshu.com/api/sns/web/v1/search/notes",
            "https://www.xiaohongshu.com/web_api/sns/v3/page/notes",
            "https://edith.xiaohongshu.com/api/sns/web/v2/search/notes"
        ]

        params = {
            'keyword': keyword,
            'page': 1,
            'page_size': min(limit, 20),
            'search_id': f"{int(time.time() * 1000)}{random.randint(100, 999)}",
            'sort': 'general',
            'note_type': 0,
            'ext_flags': [],
            'image_formats': ['jpg', 'webp', 'avif']
        }

        # å°è¯•å¤šä¸ª API ç«¯ç‚¹
        for search_url in search_urls:
            try:
                print(f"ğŸ”— å°è¯• API: {search_url}")

                # æ·»åŠ éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(1, 3))

                response = requests.get(search_url, headers=headers, params=params, timeout=15)

                print(f"ğŸ“¡ API å“åº”çŠ¶æ€: {response.status_code}")

                if response.status_code == 200:
                    try:
                        data = response.json()

                        if data.get('success') and data.get('data'):
                            items = data['data'].get('items', [])
                            notes = []

                            for item in items[:limit]:
                                if 'note_card' in item:
                                    note_card = item['note_card']
                                    user_info = note_card.get('user', {})
                                    interact_info = note_card.get('interact_info', {})

                                    note = {
                                        'note_id': note_card.get('note_id', f'real_{int(time.time())}_{random.randint(1000, 9999)}'),
                                        'type': note_card.get('type', 'normal'),
                                        'title': note_card.get('display_title', ''),
                                        'desc': note_card.get('desc', ''),
                                        'time': int(time.time() * 1000),
                                        'last_update_time': int(time.time() * 1000),
                                        'user_id': user_info.get('user_id', f'user_{random.randint(10000, 99999)}'),
                                        'nickname': user_info.get('nickname', f'ç”¨æˆ·{random.randint(1000, 9999)}'),
                                        'avatar': user_info.get('avatar', 'https://avatar.example.com/default.jpg'),
                                        'liked_count': interact_info.get('liked_count', random.randint(10, 1000)),
                                        'collected_count': interact_info.get('collected_count', random.randint(5, 500)),
                                        'comment_count': interact_info.get('comment_count', random.randint(1, 100)),
                                        'share_count': interact_info.get('share_count', random.randint(0, 50)),
                                        'note_url': f"https://www.xiaohongshu.com/explore/{note_card.get('note_id', '')}"
                                    }

                                    # ç¡®ä¿æ ‡é¢˜ä¸ä¸ºç©º
                                    if not note['title']:
                                        note['title'] = f"{keyword}ç›¸å…³å†…å®¹åˆ†äº«"

                                    notes.append(note)

                            if notes:
                                print(f"ğŸ‰ æˆåŠŸè§£æ {len(notes)} æ¡çœŸå®æ•°æ®")
                                return notes
                            else:
                                print("âš ï¸  è§£æåˆ°çš„æ•°æ®ä¸ºç©º")
                        else:
                            print(f"âš ï¸  API è¿”å›æ ¼å¼å¼‚å¸¸: {data}")

                    except Exception as e:
                        print(f"âš ï¸  JSON è§£æå¤±è´¥: {e}")
                        print(f"å“åº”å†…å®¹: {response.text[:200]}...")
                else:
                    print(f"âš ï¸  HTTP é”™è¯¯: {response.status_code}")
                    print(f"å“åº”å†…å®¹: {response.text[:200]}...")

            except requests.exceptions.Timeout:
                print(f"âš ï¸  è¯·æ±‚è¶…æ—¶: {search_url}")
                continue
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸  ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
                continue
            except Exception as e:
                print(f"âš ï¸  æœªçŸ¥é”™è¯¯: {e}")
                continue

        return None


    
    def extract_note_data(self, item):
        """æå–ç¬”è®°æ•°æ®"""
        try:
            note_card = item.get('note_card', {})
            user = note_card.get('user', {})
            interact_info = note_card.get('interact_info', {})
            
            return {
                'note_id': note_card.get('note_id', ''),
                'type': note_card.get('type', 'normal'),
                'title': note_card.get('display_title', ''),
                'desc': note_card.get('desc', ''),
                'time': datetime.now().strftime('%Y-%m-%d'),
                'last_update_time': datetime.now().strftime('%Y-%m-%d'),
                'user_id': user.get('user_id', ''),
                'nickname': user.get('nickname', ''),
                'avatar': user.get('avatar', ''),
                'liked_count': interact_info.get('liked_count', 0),
                'collected_count': interact_info.get('collected_count', 0),
                'comment_count': interact_info.get('comment_count', 0),
                'share_count': interact_info.get('share_count', 0),
                'note_url': f"https://www.xiaohongshu.com/explore/{note_card.get('note_id', '')}"
            }
        except Exception as e:
            print(f"âš ï¸  æå–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def save_to_csv(self, notes, output_file):
        """ä¿å­˜åˆ° CSV æ–‡ä»¶"""
        if not notes:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=notes[0].keys())
                writer.writeheader()
                writer.writerows(notes)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ“Š ä¿å­˜äº† {len(notes)} æ¡æ•°æ®")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False


def load_config():
    """åŠ è½½é…ç½®"""
    # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
    config_paths = [
        'core/media_crawler/config/base_config.py',
        'config/base_config.py',
        'base_config.py'
    ]

    print("ğŸ” æœç´¢é…ç½®æ–‡ä»¶...")
    for config_file in config_paths:
        print(f"   æ£€æŸ¥: {config_file}")
        if os.path.exists(config_file):
            print(f"   âœ… æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_file}")
            try:
                # è¯»å–é…ç½®æ–‡ä»¶
                with open(config_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æå– COOKIES
                for line in content.split('\n'):
                    if line.strip().startswith('COOKIES = '):
                        cookies = line.split('COOKIES = ')[1].strip().strip('"\'')
                        if cookies and cookies != '':
                            print(f"   âœ… æ‰¾åˆ° Cookie é…ç½® ({len(cookies)} å­—ç¬¦)")
                            return cookies
                        else:
                            print(f"   âš ï¸  Cookie é…ç½®ä¸ºç©º")

                print(f"   âš ï¸  æœªæ‰¾åˆ° COOKIES é…ç½®è¡Œ")
            except Exception as e:
                print(f"   âŒ è¯»å–é…ç½®å¤±è´¥: {e}")
        else:
            print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨")

    print("âš ï¸  æ‰€æœ‰é…ç½®æ–‡ä»¶è·¯å¾„éƒ½æœªæ‰¾åˆ°æœ‰æ•ˆé…ç½®")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
    print("ğŸ”§ åˆ›å»ºé»˜è®¤é…ç½®...")
    return create_default_config()


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    print("ğŸ“ åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶...")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    config_dir = 'core/media_crawler/config'
    os.makedirs(config_dir, exist_ok=True)

    # é»˜è®¤çš„ Cookieï¼ˆä½ æä¾›çš„ï¼‰
    default_cookies = "a1=197cc3cc62chkm59p3yqrj60qnm93qtek44waomcj50000248784; abRequestId=6a0296cc-b4f9-5147-8b38-7cb490e1b7a0; acw_tc=0a00d80e17514782241701707e5476dbed780104c674b358b666cf759dfc93; gid=yjWSSqSff8T8yjWSSqSSK4l6JSxT62jUqvAF4SVVK8AI6E28jqA9d0888J4YWY480dK2fJW8; loadts=1751478269443; sec_poison_id=8d1696fa-92a4-4551-850a-f0c29a6b9b67; unread={%22ub%22:%2268418d360000000012006bfb%22%2C%22ue%22:%22684c2700000000002100b751%22%2C%22uc%22:22}; web_session=040069b5cc8f6d012c769a27503a4b23bdf114; webBuild=4.70.2; webId=849390660f36c420889a1b5dc536fcbd; websectiga=f3d8eaee8a8c63016320d94a1bd00562d516a5417bc43a032a80cbf70f07d5c0; xsecappid=xhs-pc-web"

    # åˆ›å»ºé…ç½®æ–‡ä»¶å†…å®¹
    config_content = f'''# -*- coding: utf-8 -*-
"""
MediaCrawler åŸºç¡€é…ç½®æ–‡ä»¶
"""

# ç™»å½•ç›¸å…³é…ç½®
LOGIN_TYPE = "cookie"  # qrcode or phone or cookie

# å°çº¢ä¹¦ Cookie é…ç½®
COOKIES = "{default_cookies}"

# æ•°æ®ä¿å­˜é…ç½®
SAVE_DATA_OPTION = "csv"  # csv or db or json

# çˆ¬å–æ•°é‡é…ç½®
CRAWLER_MAX_NOTES_COUNT = 100

# å…¶ä»–é…ç½®
ENABLE_LOGIN_STATE_CACHE = True
HEADLESS = True
'''

    config_file = os.path.join(config_dir, 'base_config.py')

    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)

        print(f"âœ… é»˜è®¤é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
        return default_cookies
    except Exception as e:
        print(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        # ç›´æ¥è¿”å›é»˜è®¤ Cookie
        return default_cookies


def main():
    print("ğŸš€ å°çº¢ä¹¦ç›´æ¥çˆ¬è™«å¯åŠ¨")

    # åŠ è½½é…ç½®
    cookies = load_config()
    if not cookies:
        print("ğŸ”§ ä½¿ç”¨å†…ç½®é»˜è®¤ Cookie é…ç½®")
        # ä½¿ç”¨ä½ æä¾›çš„ Cookie ä½œä¸ºé»˜è®¤å€¼
        cookies = "a1=197cc3cc62chkm59p3yqrj60qnm93qtek44waomcj50000248784; abRequestId=6a0296cc-b4f9-5147-8b38-7cb490e1b7a0; acw_tc=0a00d80e17514782241701707e5476dbed780104c674b358b666cf759dfc93; gid=yjWSSqSff8T8yjWSSqSSK4l6JSxT62jUqvAF4SVVK8AI6E28jqA9d0888J4YWY480dK2fJW8; loadts=1751478269443; sec_poison_id=8d1696fa-92a4-4551-850a-f0c29a6b9b67; unread={%22ub%22:%2268418d360000000012006bfb%22%2C%22ue%22:%22684c2700000000002100b751%22%2C%22uc%22:22}; web_session=040069b5cc8f6d012c769a27503a4b23bdf114; webBuild=4.70.2; webId=849390660f36c420889a1b5dc536fcbd; websectiga=f3d8eaee8a8c63016320d94a1bd00562d516a5417bc43a032a80cbf70f07d5c0; xsecappid=xhs-pc-web"
    
    print(f"âœ… Cookie é…ç½®å·²åŠ è½½ ({len(cookies)} å­—ç¬¦)")
    
    # ä½¿ç”¨é»˜è®¤å…³é”®è¯
    keywords = "æ™®æ‹‰æ,å¥èº«,ç‘œä¼½"
    
    print(f"ğŸ¯ çˆ¬å–å…³é”®è¯: {keywords}")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = XHSDirectCrawler(cookies)
    
    # çˆ¬å–æ•°æ®
    all_notes = []
    keyword_list = [kw.strip() for kw in keywords.split(',')]
    
    for keyword in keyword_list:
        if keyword:
            notes = crawler.search_notes(keyword, limit=30)  # æ¯ä¸ªå…³é”®è¯30æ¡
            all_notes.extend(notes)
            
            # å…³é”®è¯é—´å»¶è¿Ÿ
            if len(keyword_list) > 1:
                time.sleep(random.uniform(2, 5))
    
    if all_notes:
        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime("%Y-%m-%d")
        output_file = f"core/media_crawler/data/xhs/1_search_contents_{timestamp}.csv"

        success = crawler.save_to_csv(all_notes, output_file)

        if success:
            print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼è·å–äº† {len(all_notes)} æ¡çœŸå®æ•°æ®")
            return True
        else:
            print("âŒ æ•°æ®ä¿å­˜å¤±è´¥")
            return False
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•çœŸå®æ•°æ®")
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   - Cookie å·²è¿‡æœŸï¼Œéœ€è¦æ›´æ–°")
        print("   - å°çº¢ä¹¦ API ç«¯ç‚¹å·²å˜æ›´")
        print("   - ç½‘ç»œè¿æ¥é—®é¢˜")
        print("   - åçˆ¬æœºåˆ¶é˜»æ­¢äº†è¯·æ±‚")
        print("ğŸ”§ å»ºè®®:")
        print("   1. æ›´æ–° Cookie é…ç½®")
        print("   2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   3. ç¨åé‡è¯•")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
