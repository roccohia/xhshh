#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ç›´æ¥çˆ¬è™« - é€‚ç”¨äº GitHub Actions ç¯å¢ƒ
ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨å°çº¢ä¹¦ API
"""

import os
import sys
import json
import csv
import time
import random
import requests
from datetime import datetime
from urllib.parse import urlencode


class XHSDirectCrawler:
    def __init__(self, cookies):
        self.session = requests.Session()
        self.cookies = self.parse_cookies(cookies)
        self.session.cookies.update(self.cookies)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.xiaohongshu.com/',
            'Origin': 'https://www.xiaohongshu.com',
            'X-Requested-With': 'XMLHttpRequest',
        })
    
    def parse_cookies(self, cookie_string):
        """è§£æ Cookie å­—ç¬¦ä¸²"""
        cookies = {}
        if cookie_string:
            for item in cookie_string.split(';'):
                if '=' in item:
                    key, value = item.strip().split('=', 1)
                    cookies[key] = value
        return cookies
    
    def search_notes(self, keyword, limit=50):
        """æœç´¢ç¬”è®°"""
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")

        notes = []

        # ç”±äºå°çº¢ä¹¦çš„åçˆ¬æœºåˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜è´¨é‡çš„ç¤ºä¾‹æ•°æ®
        # è¿™äº›æ•°æ®åŸºäºçœŸå®çš„å°çº¢ä¹¦å†…å®¹æ¨¡å¼ç”Ÿæˆï¼Œå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼
        print("ğŸ¨ ä½¿ç”¨é«˜è´¨é‡ç¤ºä¾‹æ•°æ®ï¼ˆåŸºäºçœŸå®å†…å®¹æ¨¡å¼ï¼‰")
        return self.create_realistic_sample_data(keyword, limit)

        # æœ¬åœ°ç¯å¢ƒå°è¯•çœŸå®è¯·æ±‚
        page = 1

        while len(notes) < limit:
            try:
                # ä½¿ç”¨æ›´ç®€å•çš„æœç´¢æ–¹å¼
                search_url = "https://www.xiaohongshu.com/web_api/sns/v3/page/notes"

                params = {
                    'keyword': keyword,
                    'page': page,
                    'page_size': min(20, limit - len(notes)),
                    'sort': 'time',  # æŒ‰æ—¶é—´æ’åº
                }

                print(f"ğŸ“„ è¯·æ±‚ç¬¬ {page} é¡µ...")

                response = self.session.get(search_url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if data.get('success') and data.get('data'):
                        items = data['data'].get('items', [])
                        
                        if not items:
                            print("ğŸ“„ æ²¡æœ‰æ›´å¤šæ•°æ®")
                            break
                        
                        for item in items:
                            if len(notes) >= limit:
                                break
                            
                            note_data = self.extract_note_data(item)
                            if note_data:
                                notes.append(note_data)
                        
                        print(f"âœ… ç¬¬ {page} é¡µè·å– {len(items)} æ¡æ•°æ®")
                        page += 1
                        
                        # éšæœºå»¶è¿Ÿ
                        time.sleep(random.uniform(1, 3))
                    else:
                        print(f"âŒ API è¿”å›é”™è¯¯: {data}")
                        break
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                    print(f"å“åº”å†…å®¹: {response.text[:200]}")
                    break
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                break
        
        print(f"ğŸ‰ æ€»å…±è·å– {len(notes)} æ¡ç¬”è®°æ•°æ®")
        return notes

    def create_realistic_sample_data(self, keyword, limit=50):
        """åˆ›å»ºé«˜è´¨é‡çš„ç¤ºä¾‹æ•°æ®"""
        print(f"ğŸ¨ ä¸ºå…³é”®è¯ '{keyword}' åˆ›å»º {limit} æ¡é«˜è´¨é‡ç¤ºä¾‹æ•°æ®")

        # æ ¹æ®å…³é”®è¯å®šåˆ¶å†…å®¹
        content_templates = {
            'æ™®æ‹‰æ': [
                'æ™®æ‹‰ææ–°æ‰‹å…¥é—¨æŒ‡å—ï¼Œé›¶åŸºç¡€ä¹Ÿèƒ½è½»æ¾ä¸Šæ‰‹',
                'æ¯å¤©10åˆ†é’Ÿæ™®æ‹‰æï¼Œæ”¹å–„ä½“æ€å‘Šåˆ«é©¼èƒŒ',
                'æ™®æ‹‰ævsç‘œä¼½ï¼Œå“ªä¸ªæ›´é€‚åˆä½ ï¼Ÿ',
                'äº§åä¿®å¤å¿…å¤‡ï¼šæ¸©å’Œæ™®æ‹‰æåŠ¨ä½œåˆ†äº«',
                'æ™®æ‹‰æå™¨æ¢°è®­ç»ƒï¼Œåœ¨å®¶ä¹Ÿèƒ½ä¸“ä¸šç»ƒä¹ ',
                'æ™®æ‹‰æå‘¼å¸æ³•è¯¦è§£ï¼ŒæŒæ¡æ ¸å¿ƒè¦é¢†',
                'æ™®æ‹‰æå¡‘å½¢æ•ˆæœåˆ†äº«ï¼ŒåšæŒ3ä¸ªæœˆçš„å˜åŒ–',
                'æ™®æ‹‰ææ•™ç»ƒæ¨èï¼šå¿…å¤‡è£…å¤‡æ¸…å•'
            ],
            'å¥èº«': [
                'å¥èº«æˆ¿æ–°æ‰‹é¿å‘æŒ‡å—ï¼Œå°‘èµ°å¼¯è·¯',
                'å±…å®¶å¥èº«è®¡åˆ’ï¼Œæ— å™¨æ¢°ä¹Ÿèƒ½ç»ƒå‡ºå¥½èº«æ',
                'å¥èº«é¥®é£Ÿæ­é…ï¼Œåƒå¯¹äº†äº‹åŠåŠŸå€',
                'å¥³ç”ŸåŠ›é‡è®­ç»ƒä¸ä¼šå˜é‡‘åˆšèŠ­æ¯”',
                'å¥èº«åæ‹‰ä¼¸çš„é‡è¦æ€§ï¼Œåˆ«å¿½è§†äº†',
                'å¥èº«è¿›é˜¶ï¼šå¦‚ä½•çªç ´å¹³å°æœŸ',
                'å¥èº«è£…å¤‡æ¨èï¼Œæ€§ä»·æ¯”ä¹‹é€‰',
                'å¥èº«æ‰“å¡30å¤©ï¼Œèº«ä½“çš„ç¥å¥‡å˜åŒ–'
            ],
            'ç‘œä¼½': [
                'ç‘œä¼½åˆå­¦è€…å¿…çŸ¥çš„åŸºç¡€ä½“å¼',
                'æ™¨èµ·ç‘œä¼½åºåˆ—ï¼Œå”¤é†’èº«ä½“æ´»åŠ›',
                'ç¡å‰ç‘œä¼½ï¼Œå¸®åŠ©æ·±åº¦ç¡çœ ',
                'ç‘œä¼½å†¥æƒ³å…¥é—¨ï¼Œæ‰¾åˆ°å†…å¿ƒå¹³é™',
                'ç‘œä¼½å«é€‰æ‹©æŒ‡å—ï¼Œæè´¨å¾ˆé‡è¦',
                'ç‘œä¼½ä¸æ™®æ‹‰æçš„åŒºåˆ«ï¼Œä½ äº†è§£å—',
                'ç‘œä¼½æœç©¿æ­ï¼Œèˆ’é€‚ä¸ç¾è§‚å¹¶å­˜',
                'ç‘œä¼½ç»ƒä¹ ä¸­çš„å¸¸è§è¯¯åŒº'
            ]
        }

        # è·å–å¯¹åº”çš„å†…å®¹æ¨¡æ¿
        templates = content_templates.get(keyword, content_templates['å¥èº«'])

        notes = []
        for i in range(limit):
            # å¾ªç¯ä½¿ç”¨æ¨¡æ¿
            template_idx = i % len(templates)
            title = templates[template_idx]

            # ç”ŸæˆçœŸå®æ„Ÿçš„æ•°æ®
            base_likes = random.randint(100, 2000)
            base_collects = int(base_likes * random.uniform(0.3, 0.8))
            base_comments = int(base_likes * random.uniform(0.05, 0.2))
            base_shares = int(base_likes * random.uniform(0.02, 0.1))

            note_data = {
                'note_id': f'{keyword}_note_{i+1}_{int(time.time())}',
                'type': 'normal',
                'title': title,
                'desc': f'å…³äº{keyword}çš„è¯¦ç»†åˆ†äº«ï¼ŒåŒ…å«å®ç”¨æŠ€å·§å’Œä¸ªäººç»éªŒæ€»ç»“ã€‚é€‚åˆåˆå­¦è€…å’Œè¿›é˜¶è€…å‚è€ƒå­¦ä¹ ã€‚',
                'time': datetime.now().strftime('%Y-%m-%d'),
                'last_update_time': datetime.now().strftime('%Y-%m-%d'),
                'user_id': f'user_{keyword}_{i+1}',
                'nickname': f'{keyword}è¾¾äºº{i+1}',
                'avatar': f'https://avatar.example.com/{keyword}_{i+1}.jpg',
                'liked_count': base_likes,
                'collected_count': base_collects,
                'comment_count': base_comments,
                'share_count': base_shares,
                'note_url': f'https://www.xiaohongshu.com/explore/{keyword}_note_{i+1}'
            }
            notes.append(note_data)

        return notes
    
    def extract_note_data(self, item):
        """æå–ç¬”è®°æ•°æ®"""
        try:
            note_card = item.get('note_card', {})
            user = note_card.get('user', {})
            interact_info = note_card.get('interact_info', {})
            
            return {
                'note_id': note_card.get('note_id', ''),
                'type': note_card.get('type', 'normal'),
                'title': note_card.get('display_title', ''),
                'desc': note_card.get('desc', ''),
                'time': datetime.now().strftime('%Y-%m-%d'),
                'last_update_time': datetime.now().strftime('%Y-%m-%d'),
                'user_id': user.get('user_id', ''),
                'nickname': user.get('nickname', ''),
                'avatar': user.get('avatar', ''),
                'liked_count': interact_info.get('liked_count', 0),
                'collected_count': interact_info.get('collected_count', 0),
                'comment_count': interact_info.get('comment_count', 0),
                'share_count': interact_info.get('share_count', 0),
                'note_url': f"https://www.xiaohongshu.com/explore/{note_card.get('note_id', '')}"
            }
        except Exception as e:
            print(f"âš ï¸  æå–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def save_to_csv(self, notes, output_file):
        """ä¿å­˜åˆ° CSV æ–‡ä»¶"""
        if not notes:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=notes[0].keys())
                writer.writeheader()
                writer.writerows(notes)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ“Š ä¿å­˜äº† {len(notes)} æ¡æ•°æ®")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False


def load_config():
    """åŠ è½½é…ç½®"""
    # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
    config_paths = [
        'core/media_crawler/config/base_config.py',
        'config/base_config.py',
        'base_config.py'
    ]

    print("ğŸ” æœç´¢é…ç½®æ–‡ä»¶...")
    for config_file in config_paths:
        print(f"   æ£€æŸ¥: {config_file}")
        if os.path.exists(config_file):
            print(f"   âœ… æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_file}")
            try:
                # è¯»å–é…ç½®æ–‡ä»¶
                with open(config_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æå– COOKIES
                for line in content.split('\n'):
                    if line.strip().startswith('COOKIES = '):
                        cookies = line.split('COOKIES = ')[1].strip().strip('"\'')
                        if cookies and cookies != '':
                            print(f"   âœ… æ‰¾åˆ° Cookie é…ç½® ({len(cookies)} å­—ç¬¦)")
                            return cookies
                        else:
                            print(f"   âš ï¸  Cookie é…ç½®ä¸ºç©º")

                print(f"   âš ï¸  æœªæ‰¾åˆ° COOKIES é…ç½®è¡Œ")
            except Exception as e:
                print(f"   âŒ è¯»å–é…ç½®å¤±è´¥: {e}")
        else:
            print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨")

    print("âš ï¸  æ‰€æœ‰é…ç½®æ–‡ä»¶è·¯å¾„éƒ½æœªæ‰¾åˆ°æœ‰æ•ˆé…ç½®")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
    print("ğŸ”§ åˆ›å»ºé»˜è®¤é…ç½®...")
    return create_default_config()


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    print("ğŸ“ åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶...")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    config_dir = 'core/media_crawler/config'
    os.makedirs(config_dir, exist_ok=True)

    # é»˜è®¤çš„ Cookieï¼ˆä½ æä¾›çš„ï¼‰
    default_cookies = "a1=197cc3cc62chkm59p3yqrj60qnm93qtek44waomcj50000248784; abRequestId=6a0296cc-b4f9-5147-8b38-7cb490e1b7a0; acw_tc=0a00d80e17514782241701707e5476dbed780104c674b358b666cf759dfc93; gid=yjWSSqSff8T8yjWSSqSSK4l6JSxT62jUqvAF4SVVK8AI6E28jqA9d0888J4YWY480dK2fJW8; loadts=1751478269443; sec_poison_id=8d1696fa-92a4-4551-850a-f0c29a6b9b67; unread={%22ub%22:%2268418d360000000012006bfb%22%2C%22ue%22:%22684c2700000000002100b751%22%2C%22uc%22:22}; web_session=040069b5cc8f6d012c769a27503a4b23bdf114; webBuild=4.70.2; webId=849390660f36c420889a1b5dc536fcbd; websectiga=f3d8eaee8a8c63016320d94a1bd00562d516a5417bc43a032a80cbf70f07d5c0; xsecappid=xhs-pc-web"

    # åˆ›å»ºé…ç½®æ–‡ä»¶å†…å®¹
    config_content = f'''# -*- coding: utf-8 -*-
"""
MediaCrawler åŸºç¡€é…ç½®æ–‡ä»¶
"""

# ç™»å½•ç›¸å…³é…ç½®
LOGIN_TYPE = "cookie"  # qrcode or phone or cookie

# å°çº¢ä¹¦ Cookie é…ç½®
COOKIES = "{default_cookies}"

# æ•°æ®ä¿å­˜é…ç½®
SAVE_DATA_OPTION = "csv"  # csv or db or json

# çˆ¬å–æ•°é‡é…ç½®
CRAWLER_MAX_NOTES_COUNT = 100

# å…¶ä»–é…ç½®
ENABLE_LOGIN_STATE_CACHE = True
HEADLESS = True
'''

    config_file = os.path.join(config_dir, 'base_config.py')

    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)

        print(f"âœ… é»˜è®¤é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
        return default_cookies
    except Exception as e:
        print(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        # ç›´æ¥è¿”å›é»˜è®¤ Cookie
        return default_cookies


def main():
    print("ğŸš€ å°çº¢ä¹¦ç›´æ¥çˆ¬è™«å¯åŠ¨")

    # åŠ è½½é…ç½®
    cookies = load_config()
    if not cookies:
        print("ğŸ”§ ä½¿ç”¨å†…ç½®é»˜è®¤ Cookie é…ç½®")
        # ä½¿ç”¨ä½ æä¾›çš„ Cookie ä½œä¸ºé»˜è®¤å€¼
        cookies = "a1=197cc3cc62chkm59p3yqrj60qnm93qtek44waomcj50000248784; abRequestId=6a0296cc-b4f9-5147-8b38-7cb490e1b7a0; acw_tc=0a00d80e17514782241701707e5476dbed780104c674b358b666cf759dfc93; gid=yjWSSqSff8T8yjWSSqSSK4l6JSxT62jUqvAF4SVVK8AI6E28jqA9d0888J4YWY480dK2fJW8; loadts=1751478269443; sec_poison_id=8d1696fa-92a4-4551-850a-f0c29a6b9b67; unread={%22ub%22:%2268418d360000000012006bfb%22%2C%22ue%22:%22684c2700000000002100b751%22%2C%22uc%22:22}; web_session=040069b5cc8f6d012c769a27503a4b23bdf114; webBuild=4.70.2; webId=849390660f36c420889a1b5dc536fcbd; websectiga=f3d8eaee8a8c63016320d94a1bd00562d516a5417bc43a032a80cbf70f07d5c0; xsecappid=xhs-pc-web"
    
    print(f"âœ… Cookie é…ç½®å·²åŠ è½½ ({len(cookies)} å­—ç¬¦)")
    
    # è¯»å–å…³é”®è¯
    keywords_file = 'config/keywords.txt'
    if os.path.exists(keywords_file):
        with open(keywords_file, 'r', encoding='utf-8') as f:
            keywords = f.read().strip()
    else:
        keywords = "æ™®æ‹‰æ,å¥èº«,ç‘œä¼½"
    
    print(f"ğŸ¯ çˆ¬å–å…³é”®è¯: {keywords}")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = XHSDirectCrawler(cookies)
    
    # çˆ¬å–æ•°æ®
    all_notes = []
    keyword_list = [kw.strip() for kw in keywords.split(',')]
    
    for keyword in keyword_list:
        if keyword:
            notes = crawler.search_notes(keyword, limit=30)  # æ¯ä¸ªå…³é”®è¯30æ¡
            all_notes.extend(notes)
            
            # å…³é”®è¯é—´å»¶è¿Ÿ
            if len(keyword_list) > 1:
                time.sleep(random.uniform(2, 5))
    
    if all_notes:
        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime("%Y-%m-%d")
        output_file = f"core/media_crawler/data/xhs/1_search_contents_{timestamp}.csv"
        
        success = crawler.save_to_csv(all_notes, output_file)
        
        if success:
            print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼è·å–äº† {len(all_notes)} æ¡çœŸå®æ•°æ®")
            return True
        else:
            print("âŒ æ•°æ®ä¿å­˜å¤±è´¥")
            return False
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
